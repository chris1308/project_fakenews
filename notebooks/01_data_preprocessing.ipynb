{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01 — Data Preprocessing\n",
        "This notebook handles:\n",
        "• Text cleaning\n",
        "• Removing stop words\n",
        "• TF-IDF vectorization"
      ],
      "metadata": {
        "id": "88uouMMmYk6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kRqEfj7XiCv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake = pd.read_csv(\"/content/Fake.csv\", on_bad_lines=\"skip\")\n",
        "true = pd.read_csv(\"/content/True.csv\", on_bad_lines=\"skip\")\n",
        "\n",
        "fake.head()\n",
        "true.head()"
      ],
      "metadata": {
        "id": "xAB4_jIrX_3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add additional label (1= fake, 0 = real)\n",
        "fake['label'] = 1\n",
        "true['label'] = 0\n",
        "\n",
        "# simplify by only keeping needed columns\n",
        "fake = fake[['text', 'label']]\n",
        "true = true[['text', 'label']]\n",
        "\n",
        "# combine fake and true\n",
        "df = pd.concat([fake, true], axis=0).reset_index(drop=True)\n",
        "\n",
        "# shuffle\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# check\n",
        "df.head()\n",
        "df.shape"
      ],
      "metadata": {
        "id": "IR_BiPWEYB6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text cleaning and preprocessing\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "1-V1GzuwYWQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining text cleaning function\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove non-alphabetic characters\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # remove stopwords & lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # join tokens back into a string\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "gLU08bcwYa86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply cleaing to dataset\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Nwnd2FOEY5eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxI9M9PrY8AC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}